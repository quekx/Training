

脏读：读到其他事务未提交的数据

事务A修改了数据但是未提交，事务B读到了修改后的数据，事务A之后又撤销了数据修改



不可重复度：事务内两次读到的数据不一致，针对同一条数据

事务A第一次读取数据，事务B提交修改了数据，事务A之后第二次读取数据，两次读取之间数据不一致



幻读：事务内两次读到的数据不一致，针对范围内新增数据

事务A第一次读取数据，事务B提交新增删除了数据，事务A之后第二次读取数据，两次读取范围的数据不一致



1. Read uncommitted 读未提交

数据随时修改，事务A和事务B操作就是没有事务控制，不加任何同步限制

修改后立刻生效，事务之间数据互相影响



2. Read committed 读提交

数据修改发生在事务提交时，只有事务提交后数据才被正式修改

大多数数据库默认隔离级别



3. Repeatable read 重复读



MVCC 快照读机制

表记录中包含三个隐含字段：主键行号 + 最近插入/修改这条记录的事务ID + 指向上一个版本的指针

数据记录的历史版本会被保存下来，存储在undo log中，每个历史版本都包含上面几个属性：主键+事务ID+指向上一个版本的指针，形成一个版本数据的链表

事务在进行**快照读**的时候会生成一个读视图：READ VIEW

读视图用来判断当前事务能够看到哪个版本的数据：可能是最新数据，也可能是undo中的历史数据



**Read View几个属性**

- `trx_ids`: 当前系统活跃(`未提交`)事务版本号集合
- `low_limit_id`: 创建当前read view 时“当前系统`最大事务版本号`+1”。
- `up_limit_id`: 创建当前read view 时“系统正处于活跃事务`最小版本号`”
- `creator_trx_id`: 创建当前read view的事务版本号；






1. redo log 
2. undo log
3. bin log



redo log 是底层存储引擎针对事务的，只有具备事务的innodb支持

为了保证事务的一致性和持久性，数据操作使用了日志预写机制（WAL），即操作日志先写入磁盘，数据可以不先写入磁盘，暂时写入缓存，然后在合适的机制统一写入磁盘。这样用于提升IO效率。redo log记录具体数据的修改。

redo log工作在数据层



undo log 和 redo log 对应，记录数据回滚操作，主要两个作用

1. 保障事务在回滚时的一致性和原子性
2. 记录了数据之前的版本，用于MVCC多版本事务查询，事务可以查询到之间的数据快照



bin log 是上层服务针对数据同步使用的，用于主从的数据同步，和底层引擎无关，无论使用何种引擎上层都提供bin log

bin log 记录了数据在逻辑上的修改，可以简单认为是执行事务的SQL语句

bin log工作在逻辑层





Retraction 撤回机制

节点状态分为 AccMode + AccRetractionMode

AccRetractionMode 代表当前节点需要关注并产生 Retraction 撤回消息机制

AccRetractionMode节点有

1. 带主键管理的 source，需要负责产生撤回消息，当某条主键的记录更新，可以看成先删DELETE后增ADD
2. 多级 KeyBy 算子，最后一级不用关注产生 Retraction
   1. 比如Join
      1. 一级join，收到DELETE只需要更新state，不用下发撤回；收到ADD，执行JOIN
      2. 二级join，收到DELETE需要下发DELETE撤回消息，因为之前下发给下级join数据已经存在state中

<img src="/Users/kaixin/Library/Application Support/typora-user-images/image-20210409112126591.png" alt="image-20210409112126591" style="zoom:33%;" />

撤回机制实现

1. 带主键管理的source，在source后面会链接KeyBy节点，记录会存储在state中，当某条记录接受时，对比state中的记录状态，下发ADD+DELETE
2. KeyBy算子，在groupBy逻辑中，针对DELETE撤回消息会作特殊的处理逻辑





### 并发度计算模型

1. 为了计算整个DAG所需的并发，我们做了如下几个假定

   1. vertex处理一条输入数据所需的时间和vertex并发度无关，和TPS也无关，即vertex处理一条记录的latency是稳定的，我们可以根据过去一段时间的耗时metric，来预估将来vertex的处理能力。
   2. 每个vertex的输入TPS和输出TPS的比例是稳定的，即上游和下游的TPS会成比例的同时变化。
   3. 不存在数据倾斜，即修改并发度可以线性增加vertex的吞吐能力。

2. 目标TPS预估

   1. source节点目标TPS预估：blink作业的source都会统计source的delay指标，通过统计作业当前的TPS，以及delay的的增长速度，我们可以预估实际TPS
      `tps_target = tps_current * (1 + delay_increasing_rate)`
      此时，source节点的目标tps的变化比例为：`tps_ratio = 1 * (1 + delay_increasing_rate)`
   2. 非source节点的目标TPS预估：按照前面分析的结论，非source的节点的目标TPS需要随他的输入节点的TPS等比例变化，即

   ```
      tps_target = input_node_target_tps / input_node_current_tps * tps_current
                <= max(input_node_tps_ratio) * tps_current
   ```

   为了保证节点具有足够的处理能力，我们取：`tps_target = max(input_node_tps_ratio) * tps_current`
   \3. 基于上述两点，我们可以递归的计算出来DAG中每个节点的目标TPS

3. 并发度计算

   1. 基于前面的分析，我们可以知道，flink作业在没有反压的情况下，每个节点是流水线中的一个worker，独立的处理数据，并输出，因此，只需要把保证每个节点的吞吐能力达到target tps，整个作业就会健康的运行
   2. 对一个给定vertex，假设其处理一条输入需要的latency为taskLatency，那么，要达到tps_target所需的并发度为：`parallelism_target = tps_target * taskLatency`
   3. 实际运行中，作业通常都会存在或多或少的反压，为了消除反压对taskLatency metric的影响，我们采用如下方法计算
      1. 统计vertex处理一条记录的耗时（包括被反压的时间）：taskLatencyPerInput
      2. 统计vertex输出一条记录被反压的时间：waitOutputPerOutput
      3. 统计vertex的输入tps：inputTps
      4. 统计vertex的输出tps：outputTps
         由此，可以得到 `taskLatency = taskLatenchPerInput - waitOutputPerOutput * outputTps / inputTps`
   4. 由于算法假设的前提是理想条件下的，实际运行时，会因为gc、checkpoint、流量抖动等原因导致vertex性能波动，因此我们会给目标parallelism乘上一个系数，默认为3倍

## 实现

1. 触发条件：
   1. source delay超过阈值触发 scale up，该参数通过作业上线时指定
   2. 有个节点的并发度，与目标并发度的比例超过指定阈值时，触发 scale down，默认为6倍
2. auto scale 过程
   1. 检查作业是否已经达到稳定状态：
      1. 所有task初始化完成，并持续运行指定时间：parallelism.scale.stable-time.ms，默认6分钟
      2. scale down时会检查checkpoint的时间，等最新的checkpoint完成后再进行scale，避免因为scale down导致延迟
   2. 采集一段时间的作业性能metric指标
   3. 根据上述算法计算各个节点的目标并发度
   4. 根据vertex 的max parallelism、job的max CU等限制，对目标并发度进行调整，保证作业能正常运行起来
   5. 触发runtime hot update
3. 一些特殊情况处理
   1. 单个节点的state不能过大，会导致磁盘压力大，state访问慢，需要基于state的大小设定节点的最小并发保护
   2. window的节点可能会出现timer数量过多导致单节点占用内存过大，需要基于timer数量设定节点的最小并发保护



