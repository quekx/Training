# 离线
#工作记录/工作总结
 [toc]

## 个人经历

## 引擎dump离线系统

### 系统职责
负责数据处理/接入，数据异构系统，为下游搜索引擎提供数据构建索引，支持在线检索

### 系统核心命题
复杂的业务场景下，支撑海量数据索引的全量构建和实时更新

### 问题
架构问题：lamda架构下传统问题，需要维护离线+实时两套计算过程，开发维护成本高
业务问题：

1. 联盟对接的上游供给数据，数据模型不统一，原始数据往往由各方DB导出，直接对接DAO层；很难在数据处理流程上统一
2. 业务数据处理复杂
以上问题加剧了数据任务的开发和维护成本

### 挑战/解法
一：lamda架构问题：lamda系统架构由离线计算+实时计算组成 -> 开发/运维效率问题
1. 升级kappa架构 -> kappa架构需要数据存储层支持全+增量统一存储，由于下游的在线搜索引擎不支持kappa架构（HA3引擎需要每天全量重建索引，原因？倒排链、索引整理、T+1数据更新），因此数据dump层无法升级至kappa架构
2. dump侧在上层进行封装，屏蔽底层物理执行引擎，为离线/实时处理流程提供统一的业务编程接口，开发只关注业务逻辑，最常见例子如SQL。通过统一的编程接口，在底层框架内适配到不同的批/流处理引擎，只需要开发一套代码，提升业务开发效率、代码可维护性

二：辅表更新问题

### 实现架构
架构图说明

1. 在开发框架层适配各种IO，ODPS/MetaQ/TT/DRC/Swift…  （IO代码示例图？）
2. 抽象灵活的编程接口，灵活的数据类型，通用的数据处理算子（Join/Combine），灵活表达数据间的处理关系（Pipeline/算子代码示例图？）
3. 全量+实时一套代码（数据处理说明？）
4. 数据逻辑单测提效（全量+实时逻辑）
	1. 算子模式：构造输入输出校验
	2. local任务模式：local本地启动任务进程，从日常环境IO读数据，测试整体作业处理流程

离线/实时在上层接口处如何统一
核心问题
1. 批流数据如何交互？Join语义例子：卖家和商品Join
	1. Join过程
	2. 实时Join读取离线数据 -> 必要性：搜索引擎使用离线数据进行全量索引构建，引擎数据状态到离线版本；后续的增量数据消息，必须从这个离线版本开始更新，否则会出错
		1. 方案1：实时Join查询外部接口：JStorm
			1. 实时数据处理依赖在线接口爬取，架构不合理，在线接口压力；外部不提供在线接口
			2. 机器/存储预算，数据更新压力qps
			3. 服务接口性能差，不适用于大数据场景
		2. 方案2：实时Join查询外部存储：Tisplus（HBase/Hologress二级索引）
			1. 外部存储维护成本高，每个Join需要维护外部存储，不够灵活 
			2. 业务处理流程复杂不统一，加剧维护成本
			3. 外部中间件资源预算
			4. 大数据存储中间件，适用于大数据场景
			5. 数据单独管理
		3. 方案3：实时Join查询本地blink state存储：DFactory
			1. state 为blink自带的内部存储机制，任务中可灵活插拔
			2. 大数据计算节点下的本地存储访问，性能高
			3. 无额外存储预算
			4. 数据管理依赖任务，不单独管理
优缺点

state状态数据的构建：依赖Blink特性
state构造原理图？

### DF 功能项
1. 通用算子封装
2. 集团通用I/O封装
3. 数据pipeline组装流程
4. trace数据sls日志
5. 限流ratelimit
6. metirc实时监控/报警
7. 任务调度封装

### 集团内工具对比
vs ODPS SQL && Blink SQL
1. 核心功能，批流数据交互的能力：业务数据状态存储、辅表更新，轻量级任务中可插拔灵活使用
2. 复杂数据处理流程下，DF更底层更灵活。SQL 适配UDF vs 直接写Data Pipeline
3. SQL不支持的功能，如blink实时state数据读写操作不对外暴露，只能从原生DataStream算子层实现（SQL只对外提供了一些基于state实现的逻辑封装：双流Join、窗口统计等）；限流；数据trace…

亮点

面向业务：对数据处理进行上层的抽象，上层只关注业务逻辑组织，尽量屏蔽数据底层细节

灵活：在抽象的同时，尽量保持处理的灵活性：与SQL相比

### 成果
离线dump任务统计
日常/大促支撑
开发效率提升

### 未来展望
1. 提升测试效率，除了任务开发效率，特别是实时处理，缺少配套数据测试工具，测试较繁琐
2. 核心开发框架产品化，提高易用性减少开发使用成本，对外输出离线/实时计算能力

## 货品挖掘打标系统

### 问题背景
业务问题
1. 生态角色繁杂众多，各个角色利益交织。官方相较于生态的优势壁垒：在于数据能力，这也是官方能提供给生态的价值，而目前官方对生态的服务能力不足。货品挖掘建立联盟货品标签效果体系，从而应用于角色货品画像，角色撮合匹配，团长扩坑等场景，提升角色对货品的感知力、转化率。

系统问题
1. 联盟侧目前缺少商品特征挖掘管理、商品标签上下行系统
2. 缺乏商品画像能力，整个生态链路中缺乏数据的串联
3. 商品挖掘的实时能力匮乏

### 系统解决方案
1. 标签上行
2. 标签下行
3. 标签效果串联
4. 标签生成

### 系统架构
架构图？
标签生成 -> 标签上行打标 -> 标签数据同步 -> 标签导购下行透出 


### 系统模块介绍
货品挖掘生成标签
1. 双十一价格实时比较商品挖掘
2. 达人实时推荐商品挖掘

IC上行打标

IC标签同步&&下行测标签召回/透出

### 未来展望
1. 货品挖掘丰富更多标签
2. 跟踪&交易侧标签效果串联，整体建立标签效果体系